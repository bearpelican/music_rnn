{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old way of pulling out corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{CONCAT_DIR}/concat_corpus.utf') as f:\n",
    "    train_contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'їPÿ\\x07{\\x919\\x05)\\x1c'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contents[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewshaw/miniconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py    \n",
    "import numpy as np    \n",
    "import json\n",
    "concat_h5 = h5py.File(f'{CONCAT_DIR}/concat_corpus.h5','r+') \n",
    "\n",
    "with open(f'{CONCAT_DIR}/concat_corpus.json', 'rb') as f:\n",
    "    concat_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = concat_h5['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a,c): \n",
    "    return np.eye(c)[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, set_type, json_file, timesteps, root_dir):\n",
    "        self.concat_h5 = h5py.File(f'{root_dir}/{h5_file}','r+')\n",
    "        self.dataset = self.concat_h5[set_type]\n",
    "        with open(f'{root_dir}/{json_file}', 'rb') as f:\n",
    "            self.concat_json = json.load(f)\n",
    "        self.vocab_size = len(self.concat_json['idx_to_token'])+1\n",
    "        self.data_length = self.dataset.shape[0]\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data_length // self.timesteps)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.timesteps\n",
    "        x = self.dataset[start:start+self.timesteps]\n",
    "        y = self.dataset[start+1:start+self.timesteps+1]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "timesteps = 64\n",
    "md = MusicDataset(h5_file='concat_corpus.h5', set_type='train', json_file='concat_corpus.json', timesteps=timesteps, root_dir=CONCAT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(md,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = enumerate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, (x, y) = next(train_iter)\n",
    "i2, (x2, y2) = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  6,  6,  6, 11,  3, 12, 13,\n",
       "        6, 14,  8, 15, 16,  6, 14,  8,  4, 16,  6, 14,  8,  9, 16,  6,  2,\n",
       "       17, 18, 19,  6,  7, 20, 21, 22,  6, 23, 20, 21, 22,  6, 24, 20, 21,\n",
       "       22,  6, 25,  3, 26, 27,  6, 28,  8, 29, 30,  6, 23,  8, 29, 30,  6,\n",
       "       24,  8, 29, 30,  6,  2, 17, 18, 31,  6,  7, 20, 21, 32,  6,  7, 20,\n",
       "        4,  5,  6,  7, 20,  9, 10,  6, 23, 26, 12, 18,  6, 24, 29],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.dataset[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_enabled = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == torch.autograd.Variable:\n",
    "        v = torch.autograd.Variable(h.data)\n",
    "        return v.cuda() if cuda_enabled else v\n",
    "    else:\n",
    "        return tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulLSTM(torch.nn.Module):\n",
    "    def __init__(self, scale_size, n_hidden, n_factors, bs, nl):\n",
    "        super().__init__()\n",
    "        self.scale_size = scale_size\n",
    "        self.nl = nl\n",
    "        self.embedding = torch.nn.Embedding(scale_size, n_factors)\n",
    "        \n",
    "        self.rnn1 = torch.nn.LSTM(n_factors, n_hidden, nl, dropout=0.5, batch_first=True)\n",
    "        self.rnn2 = torch.nn.LSTM(n_hidden, n_hidden, nl, dropout=0.5, batch_first=True)\n",
    "        self.rnn3 = torch.nn.LSTM(n_hidden, n_hidden, nl, dropout=0.5, batch_first=True)\n",
    "        \n",
    "        if cuda_enabled:\n",
    "            self.rnn1 = self.rnn1.cuda()\n",
    "            self.rnn2 = self.rnn2.cuda()\n",
    "            self.rnn3 = self.rnn3.cuda()\n",
    "        \n",
    "        self.bn1 = nn.utils.weight_norm(self.rnn1, 'weight_hh_l0')\n",
    "        self.bn1 = nn.utils.weight_norm(self.bn1, 'weight_ih_l0')\n",
    "        self.bn2 = nn.utils.weight_norm(self.rnn2, 'weight_hh_l0')\n",
    "        self.bn2 = nn.utils.weight_norm(self.bn2, 'weight_ih_l0')\n",
    "        self.bn3 = nn.utils.weight_norm(self.rnn3, 'weight_hh_l0')\n",
    "        self.bn3 = nn.utils.weight_norm(self.bn3, 'weight_ih_l0')\n",
    "        \n",
    "        # pytorch rnn does not currently work with batchnorm\n",
    "        self.l_out = torch.nn.Linear(n_hidden, scale_size)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.reset_all_hidden(bs)\n",
    "        self.bs = bs\n",
    "        \n",
    "    def forward(self, notes):\n",
    "        bs = notes.shape[0]\n",
    "        if self.h1[0].size(1) != bs: \n",
    "            self.reset_all_hidden(bs)\n",
    "        emb = self.embedding(notes)\n",
    "#         outp1,h1 = self.rnn1(emb, self.h1)\n",
    "        outp1,h1 = self.bn1(emb, self.h1)\n",
    "        outp2,h2 = self.bn2(outp1, self.h2)\n",
    "        outp3,h3 = self.bn3(outp2, self.h3)\n",
    "        self.h1 = repackage_var(h1)\n",
    "        self.h2 = repackage_var(h2)\n",
    "        self.h3 = repackage_var(h3)\n",
    "        return torch.nn.functional.log_softmax(self.l_out(outp3), dim=-1).view(-1, self.scale_size)\n",
    "#         return torch.nn.functional.log_softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "#         return torch.nn.functional.softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "    \n",
    "    def reset_all_hidden(self, bs):\n",
    "        self.h1 = self.init_hidden(bs)\n",
    "        self.h2 = self.init_hidden(bs)\n",
    "        self.h3 = self.init_hidden(bs)\n",
    "        \n",
    "    def init_hidden(self, bs):\n",
    "        h1 = torch.autograd.Variable(torch.zeros(self.nl, bs, self.n_hidden))\n",
    "        h2 = torch.autograd.Variable(torch.zeros(self.nl, bs, self.n_hidden))\n",
    "        if cuda_enabled:\n",
    "            return (h1.cuda(), h2.cuda())\n",
    "        return h1, h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StatefulLSTM(md.vocab_size, n_hidden=256, n_factors=10, bs=batch_size, nl=2)\n",
    "if cuda_enabled:\n",
    "    m = m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2+1) % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3 Loss: 4.6451921463012695\n",
      "Iteration: 6 Loss: 4.152252197265625\n",
      "Iteration: 9 Loss: 3.6559832096099854\n",
      "Iteration: 12 Loss: 3.4879775047302246\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8a9567dc74fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display_step = 3\n",
    "training_steps = 20\n",
    "for step in range(training_steps):\n",
    "# for step in tqdm(range(training_steps)):\n",
    "    for i, (data,target) in enumerate(train_loader):\n",
    "        data, target = torch.autograd.Variable(data.long()), torch.autograd.Variable(target.long())\n",
    "        if cuda_enabled:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        m.zero_grad()\n",
    "        forward = m(data)\n",
    "        loss = loss_fn(forward, target.view(-1))\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "        if ((i+1) % display_step == 0):\n",
    "            print(f'Iteration: {i+1} Loss: {loss.data[0]}')\n",
    "    print(f'Step: {step} Loss: {loss.data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'{OUT_DIR}/../models/bachbot_stackedlstm_rnn_t64.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda_enabled:\n",
    "    m.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    m.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to have unknown state 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = md.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_song = md.dataset[:timesteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(song, seq_length):\n",
    "    full_song = song.tolist()\n",
    "    # generate music!\n",
    "    m.reset_all_hidden(batch_size)\n",
    "    for i in range(seq_length):\n",
    "        seed = np.array([full_song[-timesteps:]])\n",
    "        # Use our RNN for prediction using our seed! \n",
    "        seed_v = torch.autograd.Variable(torch.from_numpy(seed).long())\n",
    "        if cuda_enabled:\n",
    "            seed_v = seed_v.cuda()\n",
    "        predict_probs = m(seed_v)\n",
    "\n",
    "#         percentage_prob = torch.exp(predict_probs)\n",
    "        # Define output vector for our generated song by sampling from our predicted probability distribution\n",
    "        \n",
    "    #     sampled_note = np.random.choice(range(md.vocab_size), p=percentage_prob[0]) # TODO\n",
    "#         sampled_note = np.argmax(percentage_prob)\n",
    "    #     print('Sampled_note:', sampled_note)\n",
    "        \n",
    "        # With multi output model, use only the last prediction. As it is predicting to n timesteps\n",
    "        v, idx = torch.max(torch.exp(predict_probs[-1]), 0)\n",
    "        full_song.append(idx.data[0])\n",
    "    return full_song\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sequence_probabilities()\n",
    "\n",
    "\n",
    "def get_x_input(partial):\n",
    "    _, _, _, seq = partial\n",
    "    input = seq[-timesteps:]\n",
    "    input_var = torch.autograd.Variable(torch.LongTensor([input]))\n",
    "    if cuda_enabled:\n",
    "        input_var = input_var.cuda()\n",
    "    return input_var\n",
    "\n",
    "# song = string\n",
    "# seq_length = generated song length\n",
    "# beam_size = what to choose from\n",
    "def beam_search(song, seq_length, beam_size):    \n",
    "    full_song = song.tolist()\n",
    "    m.reset_all_hidden(batch_size)\n",
    "    partial_sequences = [(0, 0, [], full_song)]\n",
    "    m.eval()\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        partial_sequences = find_partials(partial_sequences)\n",
    "        \n",
    "    final_sequence = partial_sequences[0][3]\n",
    "    return final_sequence\n",
    "    \n",
    "def find_partials(partial_sequences):\n",
    "    partial_next = []\n",
    "    for partial in partial_sequences:\n",
    "        it, tot_p, p_list, seq = partial\n",
    "        x_input = get_x_input(partial)\n",
    "\n",
    "        predict_probs = m(x_input)\n",
    "        # last_it_probs = torch.exp(predict_probs[-(it+1):]) # this is to predict the last few iterations\n",
    "        last_it_probs = torch.exp(predict_probs[-1:])\n",
    "        top, idxs = torch.topk(last_it_probs, beam_size, 1)\n",
    "\n",
    "        for i in range(beam_size):\n",
    "            prob = top.data[0][i]\n",
    "            idx = idxs.data[0][i]\n",
    "            new_p_list = p_list+[prob]\n",
    "            partial_next.append((it+1, np.mean(new_p_list), new_p_list, seq+[idx]))\n",
    "\n",
    "    partial_sequences = sorted(partial_next, key=lambda x: x[1], reverse=True)[:3]\n",
    "    return partial_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 3,\n",
       " 12,\n",
       " 13,\n",
       " 6,\n",
       " 14,\n",
       " 8,\n",
       " 15,\n",
       " 16,\n",
       " 6,\n",
       " 14,\n",
       " 8,\n",
       " 4,\n",
       " 16,\n",
       " 6,\n",
       " 14,\n",
       " 8,\n",
       " 9,\n",
       " 16,\n",
       " 6,\n",
       " 2,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 6,\n",
       " 7,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 6,\n",
       " 23,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 6,\n",
       " 24,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 6,\n",
       " 25,\n",
       " 3,\n",
       " 26,\n",
       " 27,\n",
       " 6,\n",
       " 28,\n",
       " 8,\n",
       " 29,\n",
       " 30,\n",
       " 6,\n",
       " 23,\n",
       " 6,\n",
       " 35,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 25]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(gen_song, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 3,\n",
       " 12,\n",
       " 13,\n",
       " 6,\n",
       " 14,\n",
       " 8,\n",
       " 15,\n",
       " 16,\n",
       " 6,\n",
       " 14,\n",
       " 8,\n",
       " 4,\n",
       " 16,\n",
       " 6,\n",
       " 14,\n",
       " 8,\n",
       " 9,\n",
       " 16,\n",
       " 6,\n",
       " 2,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 6,\n",
       " 7,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 6,\n",
       " 23,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 6,\n",
       " 24,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 6,\n",
       " 25,\n",
       " 3,\n",
       " 26,\n",
       " 27,\n",
       " 6,\n",
       " 28,\n",
       " 8,\n",
       " 29,\n",
       " 30,\n",
       " 6,\n",
       " 23,\n",
       " 6,\n",
       " 25,\n",
       " 89,\n",
       " 35,\n",
       " 38,\n",
       " 35,\n",
       " 6,\n",
       " 35,\n",
       " 6,\n",
       " 12]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence(gen_song, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = md.dataset[:md.timesteps]\n",
    "beam_size = 3\n",
    "seq_length = 4\n",
    "full_song = song.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration, total_probability, probabilities, current sequence\n",
    "partial_sequences = [(0, 0, [], full_song)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_input(partial):\n",
    "    _, _, _, seq = partial\n",
    "    input = seq[-timesteps:]\n",
    "    input_var = torch.autograd.Variable(torch.LongTensor([input]))\n",
    "    if cuda_enabled:\n",
    "        input_var = input_var.cuda()\n",
    "    return input_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pooh', 'Christopher', 'pooh', 'pooh', 'pooh'], dtype='<U11')"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(\n",
    "  ['pooh', 'rabbit', 'piglet', 'Christopher'], \n",
    "  5,\n",
    "  p=[0.5, 0.1, 0.1, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatefulLSTM(\n",
       "  (embedding): Embedding(109, 10)\n",
       "  (rnn1): LSTM(10, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (rnn2): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (rnn3): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (bn1): LSTM(10, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (bn2): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (bn3): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (l_out): Linear(in_features=256, out_features=109, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice(top, idxs):\n",
    "    return np.random.choice(\n",
    "      idxs.data.numpy().reshape(-1), \n",
    "      1,\n",
    "      p=(top/top.sum()).data.numpy().reshape(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1388281136751175"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top.data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial = partial_sequences[0]\n",
    "\n",
    "m.reset_all_hidden(batch_size)\n",
    "\n",
    "partial_next = []\n",
    "for partial in partial_sequences:\n",
    "    it, tot_p, p_list, seq = partial\n",
    "    x_input = get_x_input(partial)\n",
    "\n",
    "    predict_probs = m(x_input)\n",
    "    # last_it_probs = torch.exp(predict_probs[-(it+1):]) # this is to predict the last few iterations\n",
    "    last_it_probs = torch.exp(predict_probs[-1:])\n",
    "    top, idxs = torch.topk(last_it_probs, beam_size, 1)\n",
    "\n",
    "    for i in range(beam_size):\n",
    "        prob = top.data[0][i]\n",
    "        idx = idxs.data[0][i]\n",
    "        new_p_list = p_list+[prob]\n",
    "        partial_next.append((it+1, np.mean(new_p_list), new_p_list, seq+[idx]))\n",
    "\n",
    "partial_sequences = sorted(partial_next, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "#1 get 3 probabilities\n",
    "#2 find the probability of the whole timestep sequence\n",
    "#3 prune branches to 3\n",
    "\n",
    "\n",
    "# With multi output model, use only the last prediction. As it is predicting to n timesteps\n",
    "# v, idx = torch.max(torch.exp(predict_probs[-1]), 0)\n",
    "# torch.topk()\n",
    "# full_song.append(idx.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: is the model predicting at every timestep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial = partial_sequences[0]\n",
    "\n",
    "m.reset_all_hidden(batch_size)\n",
    "\n",
    "it, _, _, seq = partial\n",
    "x_input = get_x_input(partial)\n",
    "\n",
    "print(seq)\n",
    "predict_probs = m(x_input)\n",
    "last_it_probs = torch.exp(predict_probs) # this is to predict the last few iterations\n",
    "# last_it_probs = torch.exp(predict_probs[-1:])\n",
    "a, idxs = torch.topk(last_it_probs, 1, 1)\n",
    "idxs.view(-1).data.numpy()\n",
    "# np.random.choice(\n",
    "#   idxs.data.numpy().reshape(-1), \n",
    "#   1,\n",
    "#   p=(top/top.sum()).data.numpy().reshape(-1)\n",
    "# )\n",
    "\n",
    "#1 get 3 probabilities\n",
    "#2 find the probability of the whole timestep sequence\n",
    "#3 prune branches to 3\n",
    "\n",
    "\n",
    "# With multi output model, use only the last prediction. As it is predicting to n timesteps\n",
    "# v, idx = torch.max(torch.exp(predict_probs[-1]), 0)\n",
    "# torch.topk()\n",
    "# full_song.append(idx.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.2018  0.0695  0.0632\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       "  32  70  22\n",
       " [torch.LongTensor of size 1x3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song = []\n",
    "for i in range(seq_length):\n",
    "    for j in range(beam_size):\n",
    "        partial = partial_sequences[j]\n",
    "        x_input = get_x_input(partial)\n",
    "        \n",
    "        predict_probs = m(x_input)\n",
    "        \n",
    "        #1 get 3 probabilities\n",
    "        #2 find the probability of the whole timestep sequence\n",
    "        #3 prune branches to 3\n",
    "        \n",
    "\n",
    "        # With multi output model, use only the last prediction. As it is predicting to n timesteps\n",
    "        v, idx = torch.max(torch.exp(predict_probs[-1]), 0)\n",
    "        torch.topk()\n",
    "        full_song.append(idx.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search end - Decoding time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(output_idx):\n",
    "    idx2token = md.concat_json['idx_to_token']\n",
    "    token_list = list(map(lambda x: idx2token.get(str(x), ''), output_idx))\n",
    "    return decode_token(token_list)\n",
    "\n",
    "def decode_token(token_list):\n",
    "    if (token_list[0] != START_DELIM):\n",
    "        token_list.insert(0, START_DELIM)\n",
    "    token_str = ''.join(token_list)\n",
    "    with open(f'{SCRATCH_DIR}/utf_to_txt.json', 'r') as f:\n",
    "        utf_to_txt = json.load(f)\n",
    "    score, stream = decode.decode_string(utf_to_txt, token_str)\n",
    "    return token_str, score, stream\n",
    "\n",
    "# test = [idx2token[f'{x}'] for x in seq_arr]; test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_seed = md.dataset[:md.timesteps]\n",
    "generated_idxs = generate_sequence(song_seed, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_str, score, stream = decode_output(generated_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{SCRATCH_DIR}/utf_to_txt.json', 'r') as f:\n",
    "    utf_to_txt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = train_contents[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{SCRATCH_DIR}/BWV-400-nomask-fermatas.utf', 'r') as f:\n",
    "    test_str = f.read()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_str, score, stream = decode_token(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.elements[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = stream.write('midi', fp=f'{OUT_DIR}/testout.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = stream.write('xml', fp=f'{OUT_DIR}/testout7.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import FileLink\n",
    "FileLink('../data/bachbot/out/testout7.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music21.environment.get('musicxmlPath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music21.environment.set('music')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
