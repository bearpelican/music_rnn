{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Nietzche Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.io import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='../data/nietzsche/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n",
    "text = open(f'{PATH}nietzsche.txt').read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all philosophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to Truth, have been unskilled and unseemly methods for\\nwinning a woman? Certainly she has never allowed herself '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 85\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars.insert(0, \"\\0\")\n",
    "\n",
    "''.join(chars[1:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not gro'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2char(idx):\n",
    "    return ''.join(indices_char[i] for i in idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a,c): \n",
    "    return np.eye(c)[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot2char(idx):\n",
    "    val,idx = idx.max(dim=-1)\n",
    "    return idx2char(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, text, idx, vocab_size, timesteps):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.text = text\n",
    "        self.dataset = idx\n",
    "        self.data_length = len(idx)\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data_length // self.timesteps)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.timesteps\n",
    "        x = self.dataset[start:start+self.timesteps]\n",
    "        y = self.dataset[start+1:start+self.timesteps+1]\n",
    "#         return one_hot(x, vocab_size), np.array(y)\n",
    "        return np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "timesteps = 64\n",
    "md = TextDataset(text, idx, vocab_size, timesteps)\n",
    "# md = MusicDataset(h5_file='concat_corpus.h5', set_type='train', json_file='concat_corpus.json', timesteps=timesteps, root_dir=CONCAT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(md,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = enumerate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, (x, y) = next(train_iter)\n",
    "i2, (x2, y2) = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char(md.dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s bow! And perhaps also the arrow, the duty, and, who\\nknows? THE'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char(x2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot2char(x2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_enabled = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == torch.autograd.Variable:\n",
    "        v = torch.autograd.Variable(h.data)\n",
    "        return v.cuda() if cuda_enabled else v\n",
    "    else:\n",
    "        return tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulLSTM(torch.nn.Module):\n",
    "    def __init__(self, scale_size, n_hidden, n_factors, bs, nl, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.scale_size = scale_size\n",
    "        self.nl = nl\n",
    "        self.bidirectional = bidirectional\n",
    "        self.bmult = 2 if bidirectional else 1\n",
    "        self.embedding = torch.nn.Embedding(scale_size, n_factors)\n",
    "        \n",
    "        self.rnn1 = torch.nn.LSTM(n_factors, n_hidden, nl, dropout=0.5, batch_first=True, bidirectional=bidirectional)\n",
    "        self.rnn2 = torch.nn.LSTM(n_hidden, n_hidden*self.bmult, nl, dropout=0.5, batch_first=True, bidirectional=bidirectional)\n",
    "        self.rnn3 = torch.nn.LSTM(n_hidden, n_hidden*self.bmult, nl, dropout=0.5, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        if cuda_enabled:\n",
    "            self.rnn1 = self.rnn1.cuda()\n",
    "            self.rnn2 = self.rnn2.cuda()\n",
    "            self.rnn3 = self.rnn3.cuda()\n",
    "        \n",
    "        self.bn1 = nn.utils.weight_norm(self.rnn1, 'weight_hh_l0')\n",
    "        self.bn1 = nn.utils.weight_norm(self.bn1, 'weight_ih_l0')\n",
    "        self.bn2 = nn.utils.weight_norm(self.rnn2, 'weight_hh_l0')\n",
    "        self.bn2 = nn.utils.weight_norm(self.bn2, 'weight_ih_l0')\n",
    "        self.bn3 = nn.utils.weight_norm(self.rnn3, 'weight_hh_l0')\n",
    "        self.bn3 = nn.utils.weight_norm(self.bn3, 'weight_ih_l0')\n",
    "        \n",
    "        # pytorch rnn does not currently work with batchnorm\n",
    "        self.l_out = torch.nn.Linear(n_hidden, scale_size)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.reset_all_hidden(bs)\n",
    "        self.bs = bs\n",
    "        \n",
    "    def forward(self, notes):\n",
    "        bs = notes.shape[0]\n",
    "        if self.h1[0].size(1) != bs: \n",
    "            self.reset_all_hidden(bs)\n",
    "        emb = self.embedding(notes)\n",
    "        outp1,h1 = self.rnn1(emb, self.h1)\n",
    "#         pdb.set_trace()\n",
    "        outp2,h2 = self.bn2(outp1, self.h2)\n",
    "        outp3,h3 = self.bn3(outp2, self.h3)\n",
    "        self.h1 = repackage_var(h1)\n",
    "        self.h2 = repackage_var(h2)\n",
    "        self.h3 = repackage_var(h3)\n",
    "        return torch.nn.functional.log_softmax(self.l_out(outp3), dim=-1).view(-1, self.scale_size)\n",
    "#         return torch.nn.functional.log_softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "#         return torch.nn.functional.softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "    \n",
    "    def reset_all_hidden(self, bs):\n",
    "        self.h1 = self.init_hidden(bs)\n",
    "        self.h2 = self.init_hidden(bs)\n",
    "        self.h3 = self.init_hidden(bs)\n",
    "        \n",
    "    def init_hidden(self, bs):\n",
    "        h1 = torch.autograd.Variable(torch.zeros(self.nl*self.bmult, bs, self.n_hidden))\n",
    "        h2 = torch.autograd.Variable(torch.zeros(self.nl*self.bmult, bs, self.n_hidden))\n",
    "        if cuda_enabled:\n",
    "            return (h1.cuda(), h2.cuda())\n",
    "        return h1, h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StatefulLSTM(md.vocab_size, n_hidden=256, n_factors=10, bs=batch_size, nl=2, bidirectional=False)\n",
    "if cuda_enabled:\n",
    "    m = m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = torch.optim.Adam(m.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 Loss: 3.1001973152160645\n",
      "Step: 0 Loss: 3.00748348236084\n",
      "Iteration: 100 Loss: 3.0935916900634766\n",
      "Step: 1 Loss: 3.005417585372925\n",
      "Iteration: 100 Loss: 3.099705696105957\n",
      "Step: 2 Loss: 3.003783941268921\n",
      "Iteration: 100 Loss: 3.1032121181488037\n",
      "Step: 3 Loss: 3.0042672157287598\n",
      "Iteration: 100 Loss: 3.10378360748291\n",
      "Step: 4 Loss: 3.004380702972412\n",
      "Iteration: 100 Loss: 3.1036312580108643\n",
      "Step: 5 Loss: 3.005208730697632\n",
      "Iteration: 100 Loss: 3.102313756942749\n",
      "Step: 6 Loss: 3.0050153732299805\n",
      "Iteration: 100 Loss: 3.0706777572631836\n",
      "Step: 7 Loss: 2.828390598297119\n",
      "Iteration: 100 Loss: 2.497222661972046\n",
      "Step: 8 Loss: 2.387152671813965\n",
      "Iteration: 100 Loss: 2.342616081237793\n",
      "Step: 9 Loss: 2.4073591232299805\n",
      "Iteration: 100 Loss: 2.271070718765259\n",
      "Step: 10 Loss: 2.2498061656951904\n",
      "Iteration: 100 Loss: 2.1978893280029297\n",
      "Step: 11 Loss: 2.1730973720550537\n",
      "Iteration: 100 Loss: 2.1283209323883057\n",
      "Step: 12 Loss: 2.062391519546509\n",
      "Iteration: 100 Loss: 2.0576000213623047\n",
      "Step: 13 Loss: 1.968438982963562\n",
      "Iteration: 100 Loss: 2.010514974594116\n",
      "Step: 14 Loss: 1.9264533519744873\n",
      "Iteration: 100 Loss: 1.96132493019104\n",
      "Step: 15 Loss: 1.8696999549865723\n",
      "Iteration: 100 Loss: 1.914664387702942\n",
      "Step: 16 Loss: 1.8150444030761719\n",
      "Iteration: 100 Loss: 1.8891783952713013\n",
      "Step: 17 Loss: 1.791893720626831\n",
      "Iteration: 100 Loss: 1.8489960432052612\n",
      "Step: 18 Loss: 1.754899501800537\n",
      "Iteration: 100 Loss: 1.8260880708694458\n",
      "Step: 19 Loss: 1.7348511219024658\n",
      "Iteration: 100 Loss: 1.7946171760559082\n",
      "Step: 20 Loss: 1.7156875133514404\n",
      "Iteration: 100 Loss: 1.7544173002243042\n",
      "Step: 21 Loss: 1.6633442640304565\n",
      "Iteration: 100 Loss: 1.7381560802459717\n",
      "Step: 22 Loss: 1.7203655242919922\n",
      "Iteration: 100 Loss: 1.7120357751846313\n",
      "Step: 23 Loss: 1.6249393224716187\n",
      "Iteration: 100 Loss: 1.6918976306915283\n",
      "Step: 24 Loss: 1.5829386711120605\n",
      "Iteration: 100 Loss: 1.6763960123062134\n",
      "Step: 25 Loss: 1.5613071918487549\n",
      "Iteration: 100 Loss: 1.6639235019683838\n",
      "Step: 26 Loss: 1.5421695709228516\n",
      "Iteration: 100 Loss: 1.6580986976623535\n",
      "Step: 27 Loss: 1.5207470655441284\n",
      "Iteration: 100 Loss: 1.6251541376113892\n",
      "Step: 28 Loss: 1.5012589693069458\n",
      "Iteration: 100 Loss: 1.6181145906448364\n",
      "Step: 29 Loss: 1.5036929845809937\n",
      "Iteration: 100 Loss: 1.6067882776260376\n",
      "Step: 30 Loss: 1.4919123649597168\n",
      "Iteration: 100 Loss: 1.592555046081543\n",
      "Step: 31 Loss: 1.4861031770706177\n",
      "Iteration: 100 Loss: 1.6064327955245972\n",
      "Step: 32 Loss: 1.4565227031707764\n",
      "Iteration: 100 Loss: 1.5778065919876099\n",
      "Step: 33 Loss: 1.449478268623352\n",
      "Iteration: 100 Loss: 1.5643259286880493\n",
      "Step: 34 Loss: 1.4468508958816528\n",
      "Iteration: 100 Loss: 1.5580580234527588\n",
      "Step: 35 Loss: 1.4480233192443848\n",
      "Iteration: 100 Loss: 1.556205153465271\n",
      "Step: 36 Loss: 1.4402565956115723\n",
      "Iteration: 100 Loss: 1.5326716899871826\n",
      "Step: 37 Loss: 1.4178693294525146\n",
      "Iteration: 100 Loss: 1.5468881130218506\n",
      "Step: 38 Loss: 1.4341856241226196\n",
      "Iteration: 100 Loss: 1.5417120456695557\n",
      "Step: 39 Loss: 1.4010313749313354\n",
      "Iteration: 100 Loss: 1.5335547924041748\n",
      "Step: 40 Loss: 1.398231029510498\n",
      "Iteration: 100 Loss: 1.539082646369934\n",
      "Step: 41 Loss: 1.394316554069519\n",
      "Iteration: 100 Loss: 1.5281819105148315\n",
      "Step: 42 Loss: 1.3854889869689941\n",
      "Iteration: 100 Loss: 1.510608196258545\n",
      "Step: 43 Loss: 1.379805326461792\n",
      "Iteration: 100 Loss: 1.5036489963531494\n",
      "Step: 44 Loss: 1.3613529205322266\n",
      "Iteration: 100 Loss: 1.4897208213806152\n",
      "Step: 45 Loss: 1.3574482202529907\n",
      "Iteration: 100 Loss: 1.500510573387146\n",
      "Step: 46 Loss: 1.3700108528137207\n",
      "Iteration: 100 Loss: 1.4764117002487183\n",
      "Step: 47 Loss: 1.3553886413574219\n",
      "Iteration: 100 Loss: 1.4802088737487793\n",
      "Step: 48 Loss: 1.3669625520706177\n",
      "Iteration: 100 Loss: 1.4775688648223877\n",
      "Step: 49 Loss: 1.37119722366333\n"
     ]
    }
   ],
   "source": [
    "display_step = 100\n",
    "training_steps = 50\n",
    "for step in range(training_steps):\n",
    "# for step in tqdm(range(training_steps)):\n",
    "    for i, (data,target) in enumerate(train_loader):\n",
    "        data, target = torch.autograd.Variable(data.long()), torch.autograd.Variable(target.long())\n",
    "        if cuda_enabled:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        m.zero_grad()\n",
    "        forward = m(data)\n",
    "        loss = loss_fn(forward, target.view(-1))\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "        if ((i+1) % display_step == 0):\n",
    "            print(f'Iteration: {i+1} Loss: {loss.data[0]}')\n",
    "    print(f'Step: {step} Loss: {loss.data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'{OUT_DIR}/../models/nietzsche_stackedlstm_rnn_t64.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda_enabled:\n",
    "    m.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    m.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to have unknown state 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = md.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(song, seq_length, sample_prob=True, one_hot=False):\n",
    "    full_song = song.tolist()\n",
    "    # generate music!\n",
    "#     m.reset_all_hidden(batch_size)\n",
    "    for i in range(seq_length):\n",
    "        if one_hot:\n",
    "            seed = np.array([one_hot(full_song[-timesteps:], vocab_size)])\n",
    "        else:\n",
    "            seed = np.array([full_song[-timesteps:]])\n",
    "#         print(idx2char(full_song[-timesteps:])))\n",
    "        # Use our RNN for prediction using our seed! \n",
    "        seed_v = torch.autograd.Variable(torch.from_numpy(seed).long())\n",
    "        if cuda_enabled:\n",
    "            seed_v = seed_v.cuda()\n",
    "        predict_probs = m(seed_v).exp()\n",
    "\n",
    "        if sample_prob:\n",
    "            # Define output vector for our generated song by sampling from our predicted probability distribution        \n",
    "            sampled_note = torch.multinomial(predict_probs[-1], 1).data[0]\n",
    "            full_song.append(sampled_note)\n",
    "        else:\n",
    "            # With multi output model, use only the last prediction. As it is predicting to n timesteps\n",
    "            v, idx = torch.max(predict_probs[-1], 0)\n",
    "            full_song.append(idx.data[0])\n",
    "    return full_song\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_input(partial):\n",
    "    _, _, _, seq = partial\n",
    "    input = seq[-timesteps:]\n",
    "    input_var = torch.autograd.Variable(torch.LongTensor([input]))\n",
    "    if cuda_enabled:\n",
    "        input_var = input_var.cuda()\n",
    "    return input_var\n",
    "\n",
    "# song = string\n",
    "# seq_length = generated song length\n",
    "# beam_size = what to choose from\n",
    "def beam_search(song, seq_length, beam_size):    \n",
    "    full_song = song.tolist()\n",
    "    m.reset_all_hidden(batch_size)\n",
    "    partial_sequences = [(0, 0, [], full_song)]\n",
    "    m.eval()\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        partial_sequences = find_partials(partial_sequences, beam_size)\n",
    "        \n",
    "    final_sequence = partial_sequences[0][3]\n",
    "    return final_sequence\n",
    "    \n",
    "def find_partials(partial_sequences, beam_size, sample_prob=True):\n",
    "    partial_next = []\n",
    "    for partial in partial_sequences:\n",
    "        it, tot_p, p_list, seq = partial\n",
    "        x_input = get_x_input(partial)\n",
    "\n",
    "        predict_probs = m(x_input)\n",
    "        # last_it_probs = torch.exp(predict_probs[-(it+1):]) # this is to predict the last few iterations\n",
    "#         pdb.set_trace()\n",
    "        last_it_probs = predict_probs[-1].exp()\n",
    "        \n",
    "        \n",
    "        if sample_prob:\n",
    "            # Define output vector for our generated song by sampling from our predicted probability distribution        \n",
    "            idxs = torch.multinomial(last_it_probs, beam_size)\n",
    "            top = last_it_probs[idxs]\n",
    "        else:\n",
    "            top, idxs = torch.topk(last_it_probs, beam_size, 1)\n",
    "\n",
    "        for i in range(beam_size):\n",
    "            prob = top.data[i]\n",
    "            idx = idxs.data[i]\n",
    "            new_p_list = p_list+[prob]\n",
    "            partial_next.append((it+1, np.mean(new_p_list), new_p_list, seq+[idx]))\n",
    "\n",
    "    partial_sequences = sorted(partial_next, key=lambda x: x[1], reverse=True)[:3]\n",
    "    return partial_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice(top, idxs):\n",
    "    return np.random.choice(\n",
    "      idxs.data.numpy().reshape(-1), \n",
    "      1,\n",
    "      p=(top/top.sum()).data.numpy().reshape(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = random.randint(0, len(md.dataset)//2)\n",
    "song_seed = md.dataset[random_seed:random_seed+md.timesteps]\n",
    "# generated_idxs = generate_sequence(song_seed, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' maturing, and perfecting--the Greeks, for\\ninstance, were a nati'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in song_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_gen_idxs = beam_search(np.array(song_seed), 500, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_idxs = generate_sequence(np.array(song_seed), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' maturing, and perfecting--the Greeks, for\\ninstance, were a nation of his own came\\ndouss. Thing and mo not insluence formalaged to accentain fan moes not be too, beft uws fering to them.\"--Tut doolge for the dight hence the scirit that it is sqown\\nwilling that\\nthis\\npuch mewhin caith and the condsalness (for vewt,\" have causes. The sresent danger.\\nMheen who conctents to when a man with the fellow the sotenological e crain of advance gh everything and belevalable, leligion of sartinility.\\nI sacctine twat consers belongem sympathy) and marmar be vinws of eterna'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char(gen_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" maturing, and perfecting--the Greeks, for\\ninstance, were a nation of their personality and therefore, without that their personality in their own existence and of the existence of the religion of the weaker of the consciousness of their own existence and of their intellect of their personality of one's own exaggeration of the consciousness of one's religious and existence, that is to say, in the religion of morality of the uncondition of the most of which there are that there are that therefore, that is not himself become, and in the greatest of their weakn\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char(bs_gen_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search end - Decoding time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(output_idx):\n",
    "    idx2token = md.concat_json['idx_to_token']\n",
    "    token_list = list(map(lambda x: idx2token.get(str(x), ''), output_idx))\n",
    "    return decode_token(token_list)\n",
    "\n",
    "def decode_token(token_list):\n",
    "    if (token_list[0] != START_DELIM):\n",
    "        token_list.insert(0, START_DELIM)\n",
    "    token_str = ''.join(token_list)\n",
    "    with open(f'{SCRATCH_DIR}/utf_to_txt.json', 'r') as f:\n",
    "        utf_to_txt = json.load(f)\n",
    "    score, stream = decode.decode_string(utf_to_txt, token_str)\n",
    "    return token_str, score, stream\n",
    "\n",
    "# test = [idx2token[f'{x}'] for x in seq_arr]; test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_seed = md.dataset[:md.timesteps]\n",
    "# generated_idxs = generate_sequence(song_seed, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_gen_idxs = beam_search(np.array(song_seed), 500, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(indices_char[i] for i in bs_gen_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_idxs = generate_sequence(song_seed, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
