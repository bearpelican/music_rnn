{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old way of pulling out corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{CONCAT_DIR}/concat_corpus.utf') as f:\n",
    "    train_contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'їPÿ\\x07{\\x919\\x05)\\x1c'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contents[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np    \n",
    "import json\n",
    "concat_h5 = h5py.File(f'{CONCAT_DIR}/concat_corpus.h5','r+') \n",
    "\n",
    "concat_json = json.load(open(f'{CONCAT_DIR}/concat_corpus.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = concat_h5['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a,c): \n",
    "    return np.eye(c)[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, set_type, json_file, timesteps, root_dir):\n",
    "        self.concat_h5 = h5py.File(f'{root_dir}/{h5_file}','r+')\n",
    "        self.dataset = self.concat_h5[set_type]\n",
    "        self.concat_json = json.load(open(f'{root_dir}/{json_file}', 'rb'))\n",
    "        self.vocab_size = len(self.concat_json['idx_to_token'])+1\n",
    "        self.data_length = self.dataset.shape[0]\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "#         print((self.data_length - self.timesteps)//10)\n",
    "#         return (self.data_length - self.timesteps)//10\n",
    "        return (self.data_length // self.timesteps)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(np.arange(10)[0:8]) # example\n",
    "#         print(np.arange(10)[8])\n",
    "        # (AS) Should not have duplicate sequences. \n",
    "        # RBMs do not actually use target value, so no point in repeating next char\n",
    "#         x = self.dataset[idx:idx+timesteps]\n",
    "#         y = self.dataset[idx+timesteps]\n",
    "        \n",
    "        start = idx*self.timesteps\n",
    "        x = self.dataset[start:start+self.timesteps]\n",
    "        y = self.dataset[start+1:start+self.timesteps+1]\n",
    "#         x_hot = one_hot(x, self.vocab_size)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MusicDataset(h5_file='concat_corpus.h5', set_type='train', json_file='concat_corpus.json', timesteps=7, root_dir=CONCAT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(md,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = enumerate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, (x, y) = next(train_iter)\n",
    "i2, (x2, y2) = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  6,  6,  6, 11,  3, 12, 13,\n",
       "        6, 14,  8, 15, 16,  6, 14,  8,  4, 16,  6, 14,  8,  9, 16,  6,  2,\n",
       "       17, 18, 19,  6,  7, 20, 21, 22,  6, 23, 20, 21, 22,  6, 24, 20, 21,\n",
       "       22,  6, 25,  3, 26, 27,  6, 28,  8, 29, 30,  6, 23,  8, 29, 30,  6,\n",
       "       24,  8, 29, 30,  6,  2, 17, 18, 31,  6,  7, 20, 21, 32,  6,  7, 20,\n",
       "        4,  5,  6,  7, 20,  9, 10,  6, 23, 26, 12, 18,  6, 24, 29],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.argmax(x[11], axis=1)[:-1]\n",
    "# b = np.argmax(x[10], axis=1)[1:]\n",
    "# np.testing.assert_array_equal(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.argmax(x2[0], axis=1)[:-1]\n",
    "# b = np.argmax(x[-1], axis=1)[1:]\n",
    "# np.testing.assert_array_equal(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    return torch.autograd.Variable(h.data) if type(h) == torch.autograd.Variable else tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale size: 109\n",
      "Batch size: 32\n",
      "Timesteps: 7\n"
     ]
    }
   ],
   "source": [
    "scale_size = md.vocab_size; print('Scale size:', scale_size)\n",
    "n_hidden = 256\n",
    "nl = 1\n",
    "n_factors = 10\n",
    "bs = batch_size; print('Batch size:', batch_size)\n",
    "timesteps = md.timesteps; print('Timesteps:', timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "\n",
    "embedding = torch.nn.Embedding(scale_size, n_factors)\n",
    "rnn = torch.nn.LSTM(n_factors, n_hidden, nl, dropout=0.5, batch_first=True)\n",
    "l_out = torch.nn.Linear(n_hidden, scale_size)\n",
    "\n",
    "h1 = torch.autograd.Variable(torch.zeros(nl, bs, n_hidden))\n",
    "h2 = torch.autograd.Variable(torch.zeros(nl, bs, n_hidden))\n",
    "h = (h1, h2)\n",
    "\n",
    "\n",
    "# dropout = nn.Dropout(.5)\n",
    "l_u = nn.Linear(n_hidden, timesteps)\n",
    "# attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "# m = StatefulLSTM(md.vocab_size, n_hidden=256, n_factors=10, bs=batch_size, nl=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = x.transpose(0, 1)\n",
    "notes = torch.autograd.Variable(x.long())\n",
    "target = torch.autograd.Variable(y.long())\n",
    "\n",
    "# # batch second\n",
    "# n_bs = notes.shape[1]\n",
    "# if h[0].size(1) != n_bs: \n",
    "#     print('batch size is not same size as original:', n_bs, h[0].size(1))\n",
    "    \n",
    "# batch first\n",
    "n_bs = notes.shape[0]\n",
    "if h[0].size(1) != n_bs: \n",
    "    print('batch size is not same size as original:', n_bs, h[0].size(1))\n",
    "            \n",
    "emb = embedding(notes)\n",
    "# emb = dropout(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h[0] = last hidden layer\n",
    "# outp[-1] = last hidden layer\n",
    "\n",
    "# u = l_u(torch.cat((emb[:, 0], h[-1]), 1))\n",
    "u = l_u(h[0])\n",
    "attn_weights = F.softmax(u, dim=1)\n",
    "# attn_s = F.softmax(attn_weights)\n",
    "# print('Attn weights:', attn_weights.shape)\n",
    "\n",
    "# new_h = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                          encoder_outputs.unsqueeze(0))\n",
    "\n",
    "# output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "# output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "# output = F.relu(output)\n",
    "# output, hidden = self.gru(output, hidden)\n",
    "\n",
    "\n",
    "outp,h_new = rnn(emb, h)\n",
    "# h = repackage_var(h_new)\n",
    "# result = F.log_softmax(l_out(outp[:, -1, :]), dim=-1)\n",
    "result = F.log_softmax(l_out(outp), dim=-1).view(-1, md.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 10])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([32, 7, 256])\n",
      "torch.Size([32, 7, 109])\n",
      "torch.Size([1, 32, 256])\n"
     ]
    }
   ],
   "source": [
    "print(emb.shape)\n",
    "print(h[0].shape)\n",
    "print(outp.shape)\n",
    "print(l_out(outp).shape)\n",
    "print(h_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 1: expected 3D tensor, got 2D at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/TH/generic/THTensorMath.c:1668",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-979fb4a6b3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 1: expected 3D tensor, got 2D at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/TH/generic/THTensorMath.c:1668"
     ]
    }
   ],
   "source": [
    "torch.bmm(attn_weights.squeeze(), emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 10])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn_weights @ emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 10])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 7])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "1.00000e-02 *\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "   3.1250  3.1250  3.1250  3.1250  3.1250  3.1250  3.1250\n",
       "[torch.FloatTensor of size 1x32x7]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 256])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn_weights.squeeze() @ outp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: torch.Size([224, 109])\n",
      "Target: torch.Size([32, 7])\n"
     ]
    }
   ],
   "source": [
    "print('Result:', result.shape)\n",
    "print('Target:', target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4.6778\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(result, target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_out(outp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    return torch.autograd.Variable(h.data).cuda() if type(h) == torch.autograd.Variable else tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulLSTM(torch.nn.Module):\n",
    "    def __init__(self, scale_size, n_hidden, n_factors, bs, nl):\n",
    "        super().__init__()\n",
    "        self.scale_size = scale_size\n",
    "        self.nl = nl\n",
    "        self.embedding = torch.nn.Embedding(scale_size, n_factors)\n",
    "        self.rnn = torch.nn.LSTM(n_factors, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = torch.nn.Linear(n_hidden, scale_size)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.u = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "    def forward(self, notes):\n",
    "        bs = notes[0].shape[0]\n",
    "        if self.h[0].size(1) != bs: \n",
    "            print('batch size is not same size as original:', bs)\n",
    "            self.init_hidden(bs)\n",
    "            \n",
    "        emb = self.embedding(notes)\n",
    "        emb = self.dropout(emb)\n",
    "        \n",
    "        \n",
    "        u = self.u(torch.cat((emb[0], self.h[-1]), 1))\n",
    "        attn_weights = F.softmax(u, dim=1)\n",
    "        print('Attn weights:', attn_weights.shape)\n",
    "        \n",
    "        new_h = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        \n",
    "        outp,h = self.rnn(emb, self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return torch.nn.functional.log_softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        h1 = torch.autograd.Variable(torch.zeros(self.nl, bs, self.n_hidden))\n",
    "        h2 = torch.autograd.Variable(torch.zeros(self.nl, bs, self.n_hidden))\n",
    "        if self._cuda():\n",
    "            self.h = (h1.cuda(), h2.cuda())\n",
    "        else:\n",
    "            self.h = (h1, h2)\n",
    "            \n",
    "    def _cuda(self):\n",
    "        return next(self.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulLSTM(torch.nn.Module):\n",
    "    def __init__(self, scale_size, n_hidden, n_factors, bs, nl):\n",
    "        super().__init__()\n",
    "        self.scale_size = scale_size\n",
    "        self.nl = nl\n",
    "        self.embedding = torch.nn.Embedding(scale_size, n_factors)\n",
    "        self.rnn = torch.nn.LSTM(n_factors, n_hidden, nl, dropout=0.5, batch_first=True)\n",
    "        self.l_out = torch.nn.Linear(n_hidden, scale_size)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, notes):\n",
    "        bs = notes.shape[0]\n",
    "        if self.h[0].size(1) != bs: \n",
    "            print('batch size is not same size as original:', bs)\n",
    "            self.init_hidden(bs)\n",
    "        emb = self.embedding(notes)\n",
    "        outp,h = self.rnn(emb, self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return torch.nn.functional.log_softmax(self.l_out(outp), dim=-1).view(-1, self.scale_size)\n",
    "#         return torch.nn.functional.log_softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "#         return torch.nn.functional.softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "    \n",
    "    def init_hidden(self, bs, cuda=True):\n",
    "        h1 = torch.autograd.Variable(torch.zeros(self.nl, bs, self.n_hidden))\n",
    "        h2 = torch.autograd.Variable(torch.zeros(self.nl, bs, self.n_hidden))\n",
    "        self.h = (h1.cuda(), h2.cuda())\n",
    "#         if self._cuda():\n",
    "#             self.h = (h1.cuda(), h2.cuda())\n",
    "#         else:\n",
    "#             self.h = (h1, h2)\n",
    "            \n",
    "    def _cuda(self):\n",
    "        return next(self.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StatefulLSTM(md.vocab_size, n_hidden=256, n_factors=10, bs=batch_size, nl=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 100\n",
    "training_steps = 2\n",
    "for step in range(training_steps):\n",
    "# for step in tqdm(range(training_steps)):\n",
    "    for i, (data,target) in enumerate(train_loader):\n",
    "        data, target = torch.autograd.Variable(data.long().cuda()), torch.autograd.Variable(target.long().cuda())\n",
    "        m.zero_grad()\n",
    "        forward = m(data)\n",
    "        loss = loss_fn(forward, target.view(-1))\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "        if (i % display_step-1 == 0):\n",
    "            print(f'Iteration: {i+1} Loss: {loss.data[0]}')\n",
    "            \n",
    "    print(f'Step: {step} Loss: {loss.data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'{OUT_DIR}/../models/bachbot_vanilla_rnn.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to have unknown state 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = md.timesteps\n",
    "output_size = md.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_song = md.dataset[:timesteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(song, seq_length):\n",
    "    full_song = song.tolist()\n",
    "    # generate music!\n",
    "    m.init_hidden(batch_size)\n",
    "    for i in range(seq_length):\n",
    "        seed = np.array([full_song[-timesteps:]])\n",
    "        # Use our RNN for prediction using our seed! \n",
    "        seed_v = torch.autograd.Variable(torch.from_numpy(seed).long()).cuda()\n",
    "    #     seed_v = torch.autograd.Variable(torch.from_numpy(np.argmax(seed, axis=1)).long()).cuda() # for onehot\n",
    "        predict_probs = m(seed_v)\n",
    "\n",
    "        percentage_prob = (np.e ** predict_probs.data.cpu().numpy())\n",
    "        # Define output vector for our generated song by sampling from our predicted probability distribution\n",
    "        \n",
    "    #     sampled_note = np.random.choice(range(md.vocab_size), p=percentage_prob[0]) # TODO\n",
    "        sampled_note = np.argmax(percentage_prob)\n",
    "    #     print('Sampled_note:', sampled_note)\n",
    "        full_song.append(sampled_note)\n",
    "    return full_song\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(output_idx):\n",
    "    idx2token = md.concat_json['idx_to_token']\n",
    "    token_list = list(map(lambda x: idx2token.get(str(x), ''), output_idx))\n",
    "    return decode_token(token_list)\n",
    "\n",
    "def decode_token(token_list):\n",
    "    if (token_list[0] != START_DELIM):\n",
    "        token_list.insert(0, START_DELIM)\n",
    "    token_str = ''.join(token_list)\n",
    "    with open(f'{SCRATCH_DIR}/utf_to_txt.json', 'r') as f:\n",
    "        utf_to_txt = json.load(f)\n",
    "    score, stream = decode.decode_string(utf_to_txt, token_str)\n",
    "    return token_str, score, stream\n",
    "\n",
    "# test = [idx2token[f'{x}'] for x in seq_arr]; test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_seed = md.dataset[:md.timesteps]\n",
    "generated_idxs = generate_sequence(song_seed, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_str, score, stream = decode_output(generated_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{SCRATCH_DIR}/utf_to_txt.json', 'r') as f:\n",
    "    utf_to_txt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = train_contents[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = open(f'{SCRATCH_DIR}/BWV-400-nomask-fermatas.utf', 'r').read()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_str, score, stream = decode_token(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.elements[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = stream.write('midi', fp=f'{OUT_DIR}/testout.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = stream.write('xml', fp=f'{OUT_DIR}/testout.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import FileLink\n",
    "FileLink('../data/bachbot/out/testout.midi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
