{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old way of pulling out corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{CONCAT_DIR}/concat_corpus.utf') as f:\n",
    "    train_contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'їPÿ\\x07{\\x919\\x05)\\x1c'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contents[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np    \n",
    "import json\n",
    "concat_h5 = h5py.File(f'{CONCAT_DIR}/concat_corpus.h5','r+') \n",
    "\n",
    "concat_json = json.load(open(f'{CONCAT_DIR}/concat_corpus.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = concat_h5['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=uint8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a,c): \n",
    "    return np.eye(c)[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, set_type, json_file, timesteps, root_dir):\n",
    "        self.concat_h5 = h5py.File(f'{root_dir}/{h5_file}','r+')\n",
    "        self.dataset = self.concat_h5[set_type]\n",
    "        self.concat_json = json.load(open(f'{root_dir}/{json_file}', 'rb'))\n",
    "        self.vocab_size = len(self.concat_json['idx_to_token'])+1\n",
    "        self.data_length = self.dataset.shape[0]\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "#         print((self.data_length - self.timesteps)//10)\n",
    "#         return (self.data_length - self.timesteps)//10\n",
    "        return (self.data_length // self.timesteps)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(np.arange(10)[0:8]) # example\n",
    "#         print(np.arange(10)[8])\n",
    "        # (AS) Should not have duplicate sequences. \n",
    "        # RBMs do not actually use target value, so no point in repeating next char\n",
    "#         x = self.dataset[idx:idx+timesteps]\n",
    "#         y = self.dataset[idx+timesteps]\n",
    "        \n",
    "        start = idx*self.timesteps\n",
    "        x = self.dataset[start:start+self.timesteps]\n",
    "        y = self.dataset[start+self.timesteps]\n",
    "#         x_hot = one_hot(x, self.vocab_size)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MusicDataset(h5_file='concat_corpus.h5', set_type='train', json_file='concat_corpus.json', timesteps=15, root_dir=CONCAT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(md,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = enumerate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, (x, y) = next(train_iter)\n",
    "i2, (x2, y2) = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  6,  6,  6, 11,  3, 12, 13,\n",
       "        6, 14,  8, 15, 16,  6, 14,  8,  4, 16,  6, 14,  8,  9, 16,  6,  2,\n",
       "       17, 18, 19,  6,  7, 20, 21, 22,  6, 23, 20, 21, 22,  6, 24, 20, 21,\n",
       "       22,  6, 25,  3, 26, 27,  6, 28,  8, 29, 30,  6, 23,  8, 29, 30,  6,\n",
       "       24,  8, 29, 30,  6,  2, 17, 18, 31,  6,  7, 20, 21, 32,  6,  7, 20,\n",
       "        4,  5,  6,  7, 20,  9, 10,  6, 23, 26, 12, 18,  6, 24, 29],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.argmax(x[11], axis=1)[:-1]\n",
    "# b = np.argmax(x[10], axis=1)[1:]\n",
    "# np.testing.assert_array_equal(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.argmax(x2[0], axis=1)[:-1]\n",
    "# b = np.argmax(x[-1], axis=1)[1:]\n",
    "# np.testing.assert_array_equal(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    return torch.autograd.Variable(h.data).cuda() if type(h) == torch.autograd.Variable else tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StatefulLSTM(torch.nn.Module):\n",
    "    def __init__(self, scale_size, n_hidden, n_factors, bs, nl):\n",
    "        super().__init__()\n",
    "        self.scale_size = scale_size\n",
    "        self.nl = nl\n",
    "        self.embedding = torch.nn.Embedding(scale_size, n_factors)\n",
    "        self.rnn = torch.nn.LSTM(n_factors, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = torch.nn.Linear(n_hidden, scale_size)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, notes):\n",
    "        bs = notes[0].shape[0]\n",
    "        if self.h[0].size(1) != bs: \n",
    "            print('batch size is not same size as original:', bs)\n",
    "            self.init_hidden(bs)\n",
    "        emb = self.embedding(notes)\n",
    "        outp,h = self.rnn(emb, self.h)\n",
    "        self.h = repackage_var(h)\n",
    "#         return torch.nn.functional.log_softmax(self.l_out(outp), dim=-1).view(-1, self.scale_size)\n",
    "        return torch.nn.functional.log_softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "#         return torch.nn.functional.softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        h1 = torch.autograd.Variable(torch.zeros(self.nl, bs, self.n_hidden))\n",
    "        h2 = torch.autograd.Variable(torch.zeros(self.nl, bs, self.n_hidden))\n",
    "        if self._cuda():\n",
    "            self.h = (h1.cuda(), h2.cuda())\n",
    "        else:\n",
    "            self.h = (h1, h2)\n",
    "            \n",
    "    def _cuda(self):\n",
    "        return next(self.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = StatefulLSTM(md.vocab_size, n_hidden=256, n_factors=10, bs=batch_size, nl=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.3156449794769287\n",
      "Step: 1 Loss: 2.235625743865967\n",
      "Step: 2 Loss: 2.2792012691497803\n",
      "Step: 3 Loss: 2.2708115577697754\n"
     ]
    }
   ],
   "source": [
    "display_step = 100\n",
    "training_steps = 20\n",
    "for step in range(training_steps):\n",
    "# for step in tqdm(range(training_steps)):\n",
    "    for i, (data,target) in enumerate(train_loader):\n",
    "        data, target = torch.autograd.Variable(data.long().cuda()), torch.autograd.Variable(target.long().cuda())\n",
    "        m.zero_grad()\n",
    "        forward = m(data)\n",
    "        loss = loss_fn(forward, target)\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "#     if (step % display_step == 0):\n",
    "    print(f'Step: {step} Loss: {loss.data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_SEED_RANDOMLY = False # Use a random snippet as a seed for generating the new song.\n",
    "if GEN_SEED_RANDOMLY:\n",
    "    ind = np.random.randint(NUM_SONGS)\n",
    "else:\n",
    "    ind = 41 # \"How Deep is Your Love\" by Calvin Harris as a starting seed\n",
    "    \n",
    "gen_song = encoded_songs[ind][:timesteps].tolist() # TODO explore different (non-random) seed options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to change sequence_length to timesteps  \n",
    "Need to have unknown state 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = md.timesteps\n",
    "output_size = md.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_song = md.dataset[:timesteps].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is not same size as original: 15\n"
     ]
    }
   ],
   "source": [
    "# generate music!\n",
    "m.init_hidden(batch_size)\n",
    "for i in range(500):\n",
    "    seed = np.array([gen_song[-timesteps:]])\n",
    "    # Use our RNN for prediction using our seed! \n",
    "    '''TODO: Write an expression to use the RNN to get the probability for the next note played based on the seed.\n",
    "    Remember that we are now using the RNN for prediction, not training.'''\n",
    "    # old way\n",
    "    seed_v = torch.autograd.Variable(torch.from_numpy(seed).long()).cuda()\n",
    "\n",
    "#     seed_v = torch.autograd.Variable(torch.from_numpy(np.argmax(seed, axis=1)).long()).cuda()\n",
    "    predict_probs = m(seed_v)\n",
    "    \n",
    "    percentage_prob = (np.e ** predict_probs.data.cpu().numpy())\n",
    "    # Define output vector for our generated song by sampling from our predicted probability distribution\n",
    "    '''TODO: Sample from the predicted distribution to determine which note gets played next.\n",
    "    You can use a function from the numpy.random library to do this.\n",
    "    Hint 1: range(x) produces a list of all the numbers from 0 to x\n",
    "    Hint 2: make sure what you pass in has the \"shape\" you expect.'''\n",
    "#     sampled_note = np.random.choice(range(md.vocab_size), p=percentage_prob[0]) # TODO\n",
    "    sampled_note = np.argmax(percentage_prob)\n",
    "#     print('Sampled_note:', sampled_note)\n",
    "    gen_song.append(sampled_note)\n",
    "\n",
    "# noteStateMatrixToMidi(gen_song, name=\"generated/gen_song_0\")\n",
    "# noteStateMatrixToMidi(encoded_songs[ind], name=\"generated/base_song_0\")\n",
    "# print(\"saved generated song! seed ind: {}\".format(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_song[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2token = md.concat_json['idx_to_token']\n",
    "test = list(map(lambda x: idx2token[str(x)] if str(x) in idx2token else '', gen_song))\n",
    "# test = [idx2token[f'{x}'] for x in seq_arr]; test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = ''.join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'їPÿ\\x07{\\x919\\x05)\\x1c\\x91\\x91\\x91Ûÿ:*±Ù}Là\\x9d{}\\x91Õ)æãHBÈÔ%\\x91H~\\x91+~\\x91\\x84%\\x91\\x91,æ\\x1c%\\x91\\x84\\x91\\x84~\\x91ê\\x8f\\x91\\x05àÔîä\\x91\\x84\\x91¤t\\x9a\\x91ï\\x1cHh%p\\x91æí\\x91\\x84\\x91)\\x9a\\x8f\\x91ÿ)êúèä\\x91àµæè%\\x91áï{\\x91\\x84\\x91\\x84\\x91þ\\x1cä\\x05\\x91\\x91\\x84\\x91\\x84\\x91ÿí\\x84\\x91Õ{~Lä\\x91¤ïÊî)èãBÙ\\x91\\x84\\x91îtÈ\\x91ÕÔ\\x91\\x84\\x91Õ\\x9a\\x91\\x9d\\x07±\\x91\\x84kæ\\x8f\\x91áïæ\\x9a\\x91\\x84\\x91\\x84t\\x9a\\x91)è\\x1c{kt\\x8fÙ\\x91\\x05h\\x91\\x84\\x91){B\\x1cèäú\\x91\\x05){~\\x91\\x91ÿµ%\\x91\\x84\\x91\\x91Õ\\x1c\\x1c\\x91\\x91tL\\x91\\x84\\x91\\x05ÐB\\x91,î)ì\\x9a\\x91Õ)\\x07+\\x1cHBè~\\x91ïæ\\x91ïµè\\x91\\x84±\\x91\\x91ï{\\x91\\x17Õ\\x9e{HïãÐ\\x11\\x91\\x84\\x91\\x17Õït\\x9a\\x91\\x84L\\x91),\\x9eã\\x9d\\x1cH\\x9eÈ\\x91\\x91æ\\x07ãÿú\\x91\\x05ïæÈ\\x91\\x84\\x91\\x05\\x1c\\x9a\\x91\\x84L\\x91\\x84Ù\\x91æí\\x91\\x17p\\x91\\x84\\x91\\x84Ù%\\x91\\x84\\x91î\\x07{\\x9a\\x91ïè\\x91\\x84þÕBÕ)Ðãµîúÿ~\\x91\\x84\\x91\\x84\\x91\\x84äp\\x91\\x05\\x07\\x1cúàî\\x9e,ïtèp\\x91æäÕê\\x91\\x84\\x91\\x84\\x91.úÕ))L\\x9a,+\\x07äÕ\\x1c\\x91\\x17ï)~\\x91\\x91\\x91Õæ)æ\\x9dï\\x07í\\x91\\x84\\x91ç\\x05ÐÕ){ãhBè±ä\\x91)t\\x91\\x91\\x05ïæL\\x91\\x84\\x91Ð\\x9d\\x91\\x17ï\\x07HBí\\x91\\x84\\x91çæ\\x07{ãÈäïæk\\x07~\\x91àæîêL\\x91Ùä\\x91\\x05ã~\\x91çà)t\\x9a%\\x91.\\x1cÙ\\x91þ{ëB%\\x91\\x05)\\x91\\x9eHÊ\\x1cÙ,ê±\\x91\\x84\\x91àæhµ\\x9d\\x91Pt.){hk\\x91\\x84\\x91\\x84\\x91'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{SCRATCH_DIR}/utf_to_txt.json', 'r') as f:\n",
    "    utf_to_txt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = ''.join(train_contents[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = open(f'{SCRATCH_DIR}/BWV-400-nomask-fermatas.utf', 'r').read()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, m21s = decode.decode_string(utf_to_txt, test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "SubConverterFileIOException",
     "evalue": "png file of xml not found. Is your file >999 pages?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSubConverterFileIOException\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-249-0ece663ade81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm21s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/music/lib/python3.6/site-packages/music21/stream/__init__.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misSorted\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoSort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m#---------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/music/lib/python3.6/site-packages/music21/base.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, fmt, app, **keywords)\u001b[0m\n\u001b[1;32m   2627\u001b[0m                                  \u001b[0mapp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m                                  \u001b[0msubformats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubformats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m                                  **keywords)\n\u001b[0m\u001b[1;32m   2630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m     \u001b[0;31m#--------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/music/lib/python3.6/site-packages/music21/converter/subConverters.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, obj, fmt, app, subformats, **keywords)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 fp = helperSubConverter.write(s, helperFormat,\n\u001b[0;32m--> 353\u001b[0;31m                                               subformats=helperSubformats, **keywords)\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhelperSubformats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/music/lib/python3.6/site-packages/music21/converter/subConverters.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, obj, fmt, fp, subformats, **keywords)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'pdf'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubformats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 and environLocal['musescoreDirectPNGPath'] != '/skip'):\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunThroughMusescore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubformats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/music/lib/python3.6/site-packages/music21/converter/subConverters.py\u001b[0m in \u001b[0;36mrunThroughMusescore\u001b[0;34m(self, fp, subformats, **keywords)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubformatExtension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindPNGfpFromXMLfp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfpOut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/music/lib/python3.6/site-packages/music21/converter/subConverters.py\u001b[0m in \u001b[0;36mfindPNGfpFromXMLfp\u001b[0;34m(self, xmlFilePath)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mpngfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmlFilePath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmlFilePath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-001.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mSubConverterFileIOException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"png file of xml not found. Is your file >999 pages?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpngfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSubConverterFileIOException\u001b[0m: png file of xml not found. Is your file >999 pages?"
     ]
    }
   ],
   "source": [
    "m21s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, [(67, False), (60, False), (52, False), (48, False)]),\n",
       " (False, [(67, True), (60, True), (52, True), (48, True)]),\n",
       " (False, []),\n",
       " (False, []),\n",
       " (False, [(64, False), (60, False), (55, False), (48, False)]),\n",
       " (False, [(64, True), (60, True), (55, True), (48, True)]),\n",
       " (False, []),\n",
       " (False, []),\n",
       " (False, [(69, False), (60, False), (53, False), (41, False)]),\n",
       " (False, [(69, True), (60, True), (53, True), (41, True)]),\n",
       " (False, [(69, True), (60, True), (52, False), (41, True)]),\n",
       " (False, [(69, True), (60, True), (52, True), (41, True)]),\n",
       " (True, [(67, False), (59, False), (50, False), (41, False)]),\n",
       " (True, [(67, True), (59, True), (50, True), (41, True)]),\n",
       " (True, []),\n",
       " (True, []),\n",
       " (True, []),\n",
       " (True, []),\n",
       " (True, []),\n",
       " (True, []),\n",
       " (False, [(60, False), (55, False), (52, False)]),\n",
       " (False, [(60, True), (55, True), (52, True)]),\n",
       " (False, []),\n",
       " (False, []),\n",
       " (False, [(62, False), (59, False), (53, False), (50, False)]),\n",
       " (False, [(62, True), (59, True), (53, True), (50, True)]),\n",
       " (False, [(62, True), (57, False), (53, True), (48, False)]),\n",
       " (False, [(62, True), (57, True), (53, True), (48, True)]),\n",
       " (False, [(62, True), (55, False), (53, True), (47, False)]),\n",
       " (False, [(62, True), (55, True), (53, True), (47, True)]),\n",
       " (False, []),\n",
       " (False, []),\n",
       " (False, [(64, False), (55, True), (52, False), (48, False)]),\n",
       " (False, [(64, True), (55, True), (52, True), (48, True)]),\n",
       " (False, []),\n",
       " (False, []),\n",
       " (False, [(65, False), (62, False), (57, False), (48, True)]),\n",
       " (False, [(65, True), (62, True), (57, True), (48, True)]),\n",
       " (False, []),\n",
       " (False, []),\n",
       " (False, [(65, True), (62, True), (55, False), (47, False)]),\n",
       " (False, [(65, True), (62, True), (55, True), (47, True)]),\n",
       " (False, [(65, True), (62, True), (55, True), (45, False)]),\n",
       " (False, [(65, True), (62, True), (55, True), (45, True)]),\n",
       " (False, [(67, False), (62, False), (55, False), (47, False)]),\n",
       " (False, [(67, True), (62, True), (55, True), (47, True)]),\n",
       " (False, []),\n",
       " (False, []),\n",
       " (False, [(64, False), (60, False), (55, False), (48, False)]),\n",
       " (False, [(64, True), (60, True), (55, True), (48, True)]),\n",
       " (False, []),\n",
       " (False, []),\n",
       " (True, [(60, False), (55, False), (52, False), (36, False)]),\n",
       " (True, [(60, True), (55, True), (52, True), (36, True)]),\n",
       " (True, []),\n",
       " (True, []),\n",
       " (True, []),\n",
       " (True, [])]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = m21s.write('midi', fp=f'{OUT_DIR}/testout3.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../data/bachbot/out/testout.midi' target='_blank'>../data/bachbot/out/testout.midi</a><br>"
      ],
      "text/plain": [
       "/home/paperspace/music_rnn/data/bachbot/out/testout.midi"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import FileLink\n",
    "FileLink('../data/bachbot/out/testout.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
