{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from util.midi_manipulation import noteStateMatrixToMidi, midiToNoteStateMatrix, lowerBound, upperBound\n",
    "from util.util import print_progress\n",
    "from util.create_dataset import create_dataset, get_batch, make_one_hot_notes\n",
    "\n",
    "import glob\n",
    "import midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_NOTES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_onehot(encoded_song):\n",
    "    reversed_encoded = np.zeros((encoded_song.shape[0], MAX_CONCURRENT_NOTES))\n",
    "    for idx, timestamp in enumerate(encoded_song):\n",
    "        note_idxs = np.where(timestamp == 1)[0]\n",
    "        if len(note_idxs) > MAX_CONCURRENT_NOTES:\n",
    "            note_idxs = note_idxs[:MAX_CONCURRENT_NOTES]\n",
    "        reversed_encoded[idx, :len(note_idxs)] = note_idxs\n",
    "    return reversed_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nonhot_dataset(min_length, multi=True):\n",
    "    songs = glob.glob('data/*.mid*')\n",
    "    \n",
    "    encoded_songs = []\n",
    "    discarded = 0\n",
    "    for song in songs:\n",
    "        encoded_song = midiToNoteStateMatrix(song)\n",
    "        if len(encoded_song) >= min_length:\n",
    "            if multi:\n",
    "                encoded_song = reverse_onehot(encoded_song)\n",
    "            else:\n",
    "                encoded_song = make_one_hot_notes(encoded_song)\n",
    "                encoded_song = np.argmax(encoded_song, axis=1)\n",
    "            encoded_songs.append(encoded_song)\n",
    "        else:\n",
    "            discarded += 1\n",
    "    print(\"{} songs processed\".format(len(songs)))\n",
    "    print(\"{} songs discarded\".format(discarded))\n",
    "    return encoded_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 songs processed\n",
      "15 songs discarded\n"
     ]
    }
   ],
   "source": [
    "min_song_length  = 128\n",
    "encoded_songs    = create_nonhot_dataset(min_song_length, multi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network Parameters\n",
    "# input_size       = encoded_songs[0].shape[1]   # The number of possible MIDI Notes\n",
    "input_size       = upperBound - lowerBound   # The number of possible MIDI Notes\n",
    "scale_size = input_size # 78\n",
    "output_size      = input_size                  # Same as input size\n",
    "hidden_size      = 256                         # Number of neurons in hidden layer\n",
    "\n",
    "learning_rate    = 0.001 # Learning rate of the model\n",
    "training_steps   = 5000  # Number of batches during training\n",
    "batch_size       = 128    # Number of songs per batch\n",
    "timesteps        = 64    # Length of song snippet -- this is what is fed into the model\n",
    "\n",
    "assert timesteps < min_song_length\n",
    "\n",
    "n_hidden = hidden_size\n",
    "\n",
    "n_factors = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_simult = [np.max(np.sum(song, 1)) for song in encoded_songs]\n",
    "# print(max(max_simult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_songs[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoded_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = glob.glob('data/*.mid*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/Feel So Close - Verse.midi'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = midi.read_midifile(songs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_song = midiToNoteStateMatrix(songs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_batch(encoded_songs, batch_size, timesteps, input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_song[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    return torch.autograd.Variable(h.data).cuda() if type(h) == torch.autograd.Variable else tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulLSTM(torch.nn.Module):\n",
    "    def __init__(self, scale_size, n_factors, bs, nl):\n",
    "        super().__init__()\n",
    "        self.scale_size = scale_size\n",
    "        self.nl = nl\n",
    "        self.embedding = torch.nn.Embedding(scale_size, n_factors)\n",
    "        self.rnn = torch.nn.LSTM(n_factors, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = torch.nn.Linear(n_hidden, scale_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, notes):\n",
    "        bs = notes[0].shape[0]\n",
    "        if self.h[0].size(1) != bs: \n",
    "            print('batch size is not same size as original:', bs)\n",
    "            self.init_hidden(bs)\n",
    "        emb = self.embedding(notes)\n",
    "        outp,h = self.rnn(emb, self.h)\n",
    "        self.h = repackage_var(h)\n",
    "#         return torch.nn.functional.log_softmax(self.l_out(outp), dim=-1).view(-1, self.scale_size)\n",
    "        return torch.nn.functional.log_softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "#         return torch.nn.functional.softmax(self.l_out(outp[:, -1, :]), dim=-1)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        h1 = torch.autograd.Variable(torch.zeros(self.nl, bs, n_hidden))\n",
    "        h2 = torch.autograd.Variable(torch.zeros(self.nl, bs, n_hidden))\n",
    "        if self._cuda():\n",
    "            self.h = (h1.cuda(), h2.cuda())\n",
    "        else:\n",
    "            self.h = (h1, h2)\n",
    "    def _cuda(self):\n",
    "        return next(self.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_enabled = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batch(encoded_songs, batch_size, timesteps, input_size, output_size):\n",
    "    rand_song_indices = np.random.randint(len(encoded_songs), size=batch_size)\n",
    "#     batch_x = np.zeros((batch_size, timesteps, input_size))\n",
    "    batch_x = np.zeros((batch_size, timesteps))\n",
    "    batch_y = np.zeros((batch_size))\n",
    "    for i in range(batch_size):\n",
    "        song_ind = rand_song_indices[i]\n",
    "        start_ind = np.random.randint(encoded_songs[song_ind].shape[0]-timesteps-1)\n",
    "        batch_x[i] = encoded_songs[song_ind][start_ind:start_ind+timesteps]\n",
    "        batch_y[i] = encoded_songs[song_ind][start_ind+timesteps]\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_cuda(*args):\n",
    "    batch_x, batch_y = get_batch(*args)\n",
    "#     y_reverse_hot = batch_y.argmax(axis=1)\n",
    "    y_v = torch.autograd.Variable(torch.from_numpy(batch_y).long())\n",
    "    x_v = torch.autograd.Variable(torch.from_numpy(batch_x).long())\n",
    "    if cuda_enabled:\n",
    "        return x_v.cuda(), y_v.cuda()\n",
    "    return x_v, y_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StatefulLSTM(scale_size, n_factors, batch_size, 2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_fn = torch.optim.Adam(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = get_batch(encoded_songs, batch_size, timesteps, input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41., 43., 52., 45., 43., 41., 43., 43., 43., 43., 43., 43., 48.,\n",
       "       36., 43., 40., 41., 48., 45., 40., 43., 40., 40., 41., 52., 44.,\n",
       "       43., 43., 40., 41., 41., 43., 43., 41., 43., 60., 41., 43., 43.,\n",
       "       43., 40., 44., 43., 45., 43., 41., 45., 41., 53., 40., 41., 43.,\n",
       "       43., 43., 43., 43., 43., 40., 43., 43., 43., 43., 43., 41., 43.,\n",
       "       43., 43., 43., 50., 43., 43., 44., 43., 41., 50., 41., 40., 43.,\n",
       "       40., 43., 50., 41., 41., 41., 43., 40., 57., 57., 41., 41., 50.,\n",
       "       43., 43., 52., 36., 43., 41., 43., 43., 43., 43., 43., 43., 43.,\n",
       "       43., 45., 43., 43., 43., 43., 41., 40., 40., 40., 43., 41., 40.,\n",
       "       48., 43., 43., 48., 43., 43., 52., 43., 43., 43., 43.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = get_batch_cuda(encoded_songs, batch_size, timesteps, input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(batch_x.shape)\n",
    "print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is not same size as original: 64\n"
     ]
    }
   ],
   "source": [
    "# vbatch_y = torch.autograd.Variable(torch.from_numpy(y_reverse_hot).long())\n",
    "forward = m(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forward[:, -1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp, targ):\n",
    "    sl,bs,nh = inp.size()\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    return torch.nn.functional.nll_loss(inp.view(-1,nh), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(forward, batch_y)\n",
    "loss.backward()\n",
    "optimizer_fn.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is not same size as original: 64\n",
      "Step: 0 Loss: 4.367828845977783\n",
      "Step: 100 Loss: 2.2450788021087646\n",
      "Step: 200 Loss: 2.084689140319824\n",
      "Step: 300 Loss: 1.9295320510864258\n",
      "Step: 400 Loss: 1.7988028526306152\n",
      "Step: 500 Loss: 1.6068980693817139\n",
      "Step: 600 Loss: 1.4209301471710205\n",
      "Step: 700 Loss: 1.3277291059494019\n",
      "Step: 800 Loss: 1.0905829668045044\n",
      "Step: 900 Loss: 0.9884465336799622\n",
      "Step: 1000 Loss: 0.9860960245132446\n",
      "Step: 1100 Loss: 1.1194413900375366\n",
      "Step: 1200 Loss: 0.7504417896270752\n",
      "Step: 1300 Loss: 0.9953158497810364\n",
      "Step: 1400 Loss: 0.9345996379852295\n",
      "Step: 1500 Loss: 0.996393620967865\n",
      "Step: 1600 Loss: 1.102683663368225\n",
      "Step: 1700 Loss: 0.9701059460639954\n",
      "Step: 1800 Loss: 0.876598060131073\n",
      "Step: 1900 Loss: 0.9252856969833374\n",
      "Step: 2000 Loss: 1.0340992212295532\n",
      "Step: 2100 Loss: 0.7221037149429321\n",
      "Step: 2200 Loss: 0.947524905204773\n",
      "Step: 2300 Loss: 0.9167802333831787\n",
      "Step: 2400 Loss: 0.8938955068588257\n",
      "Step: 2500 Loss: 0.9652553796768188\n",
      "Step: 2600 Loss: 0.7662796974182129\n",
      "Step: 2700 Loss: 0.8213979601860046\n",
      "Step: 2800 Loss: 0.9163334965705872\n",
      "Step: 2900 Loss: 0.7862563729286194\n",
      "Step: 3000 Loss: 0.9541436433792114\n",
      "Step: 3100 Loss: 0.6358964443206787\n",
      "Step: 3200 Loss: 0.8500671982765198\n",
      "Step: 3300 Loss: 1.3054018020629883\n",
      "Step: 3400 Loss: 0.8112098574638367\n",
      "Step: 3500 Loss: 0.8479722738265991\n",
      "Step: 3600 Loss: 0.7096540927886963\n",
      "Step: 3700 Loss: 0.7890239953994751\n",
      "Step: 3800 Loss: 0.9144303798675537\n",
      "Step: 3900 Loss: 0.6106129884719849\n",
      "Step: 4000 Loss: 0.7764891386032104\n",
      "Step: 4100 Loss: 0.9609630107879639\n",
      "Step: 4200 Loss: 0.8097212910652161\n",
      "Step: 4300 Loss: 0.7207272052764893\n",
      "Step: 4400 Loss: 0.8284045457839966\n",
      "Step: 4500 Loss: 0.7203359603881836\n",
      "Step: 4600 Loss: 1.063347578048706\n",
      "Step: 4700 Loss: 0.7221865653991699\n",
      "Step: 4800 Loss: 0.6100680828094482\n",
      "Step: 4900 Loss: 0.8107455372810364\n"
     ]
    }
   ],
   "source": [
    "display_step = 100\n",
    "for step in range(training_steps):\n",
    "# for step in tqdm(range(training_steps)):\n",
    "    batch_x, batch_y = get_batch_cuda(encoded_songs, batch_size, timesteps, input_size, output_size)\n",
    "#     y_reverse_hot = batch_y.argmax(axis=1)\n",
    "#     vbatch_y = torch.autograd.Variable(torch.from_numpy(y_reverse_hot).long())\n",
    "    m.zero_grad()\n",
    "    forward = m(batch_x)\n",
    "    loss = loss_fn(forward, batch_y)\n",
    "    loss.backward()\n",
    "    optimizer_fn.step()\n",
    "    if (step % display_step == 0):\n",
    "        print(f'Step: {step} Loss: {loss.data[0]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m(batch_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_SEED_RANDOMLY = False # Use a random snippet as a seed for generating the new song.\n",
    "if GEN_SEED_RANDOMLY:\n",
    "    ind = np.random.randint(NUM_SONGS)\n",
    "else:\n",
    "    ind = 41 # \"How Deep is Your Love\" by Calvin Harris as a starting seed\n",
    "    \n",
    "gen_song = encoded_songs[ind][:timesteps].tolist() # TODO explore different (non-random) seed options\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def evaluate(seed):\n",
    "# encoded_songs[ind][:timesteps].tolist()\n",
    "len(gen_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to have an unknown state (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def noteStateMatrixToMidi(statematrix, name=\"example\", span=scale_size):\n",
    "#     statematrix = np.array(statematrix)\n",
    "    pattern = midi.Pattern()\n",
    "    track = midi.Track()\n",
    "    pattern.append(track)\n",
    "\n",
    "    span = upperBound-lowerBound\n",
    "    tickscale = 55\n",
    "\n",
    "    lastcmdtime = 0\n",
    "    prevstate = 0\n",
    "    for time, state in enumerate(statematrix):  \n",
    "        offNotes = []\n",
    "        onNotes = []\n",
    "        if prevstate != state:\n",
    "            offNotes.append(prevstate)\n",
    "            onNotes.append(state)\n",
    "#         elif state > 0:\n",
    "        prevstate = state\n",
    "        for note in offNotes:\n",
    "            track.append(midi.NoteOffEvent(tick=(time-lastcmdtime)*tickscale, pitch=note+lowerBound))\n",
    "            lastcmdtime = time\n",
    "        for note in onNotes:\n",
    "            track.append(midi.NoteOnEvent(tick=(time-lastcmdtime)*tickscale, velocity=40, pitch=note+lowerBound))\n",
    "            lastcmdtime = time\n",
    "\n",
    "        prevstate = state\n",
    "\n",
    "    eot = midi.EndOfTrackEvent(tick=1)\n",
    "    track.append(eot)\n",
    "\n",
    "    midi.write_midifile(\"{}.mid\".format(name), pattern)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is not same size as original: 64\n",
      "saved generated song! seed ind: 41\n"
     ]
    }
   ],
   "source": [
    "# generate music!\n",
    "m.init_hidden(batch_size)\n",
    "for i in range(500):\n",
    "    seed = np.array([gen_song[-timesteps:]])\n",
    "    # Use our RNN for prediction using our seed! \n",
    "    '''TODO: Write an expression to use the RNN to get the probability for the next note played based on the seed.\n",
    "    Remember that we are now using the RNN for prediction, not training.'''\n",
    "    # old way\n",
    "    seed_v = torch.autograd.Variable(torch.from_numpy(seed).long()).cuda()\n",
    "\n",
    "#     seed_v = torch.autograd.Variable(torch.from_numpy(np.argmax(seed, axis=1)).long()).cuda()\n",
    "    predict_probs = m(seed_v)\n",
    "    \n",
    "    percentage_prob = (np.e ** predict_probs.data.cpu().numpy())\n",
    "    # Define output vector for our generated song by sampling from our predicted probability distribution\n",
    "    played_notes = np.zeros(output_size)\n",
    "    '''TODO: Sample from the predicted distribution to determine which note gets played next.\n",
    "    You can use a function from the numpy.random library to do this.\n",
    "    Hint 1: range(x) produces a list of all the numbers from 0 to x\n",
    "    Hint 2: make sure what you pass in has the \"shape\" you expect.'''\n",
    "    sampled_note = np.random.choice(range(output_size), p=percentage_prob[0]) # TODO\n",
    "#     print('Sampled_note:', sampled_note)\n",
    "#     played_notes[sampled_note] = 1\n",
    "#     gen_song.append(played_notes)\n",
    "    gen_song.append(sampled_note)\n",
    "\n",
    "noteStateMatrixToMidi(gen_song, name=\"generated/gen_song_0\")\n",
    "noteStateMatrixToMidi(encoded_songs[ind], name=\"generated/base_song_0\")\n",
    "print(\"saved generated song! seed ind: {}\".format(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='generated/gen_song_0.mid' target='_blank'>generated/gen_song_0.mid</a><br>"
      ],
      "text/plain": [
       "/home/paperspace/introtodeeplearning_labs/lab1/generated/gen_song_0.mid"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(\"generated/gen_song_0.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(\"generated/base_song_0.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.array([gen_song[-timesteps:]])\n",
    "#     predict_probs = sess.run(prediction, feed_dict={input_vec: seed}) # TODO\n",
    "seed_v = torch.autograd.Variable(torch.from_numpy(seed).long()).cuda()\n",
    "\n",
    "# seed_v = torch.autograd.Variable(torch.from_numpy(np.argmax(seed, axis=1)).long()).cuda()\n",
    "print(seed_v.size())\n",
    "predict_probs = m(seed_v)\n",
    "\n",
    "print(predict_probs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(predict_probs.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_prob = (np.e ** predict_probs.data.cpu().numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_note = np.random.choice(range(output_size), p=percentage_prob) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
